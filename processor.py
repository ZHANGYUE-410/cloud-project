"""
æ•°æ®å¤„ç†æ¨¡å— - æ›´æ–°ç‰ˆï¼ˆæ”¯æŒå¤šæ•°æ®ç±»å‹ï¼‰
"""
import pandas as pd
import json
from datetime import datetime
import os
import numpy as np

def load_data():
    """åŠ è½½æ‰€æœ‰ç±»å‹çš„æ•°æ®"""
    print("ğŸ“‚ åŠ è½½æ•°æ®æ–‡ä»¶...")    
    data_files = {
        "books": "data/raw/books.csv",
        "courses": "data/raw/courses.csv", 
        "news": "data/raw/news.csv",
        "notices": "data/raw/notices.csv"  # æ–°å¢
    }    
    loaded_data = {}   
    for data_type, file_path in data_files.items():
        try:
            if os.path.exists(file_path):
                df = pd.read_csv(file_path, encoding='utf-8')
                loaded_data[data_type] = df
                print(f"âœ… åŠ è½½ {data_type}: {len(df)}æ¡")
            else:
                print(f"âš ï¸ æ–‡ä»¶ä¸å­˜åœ¨: {file_path}")
                loaded_data[data_type] = pd.DataFrame()
        except Exception as e:
            print(f"âŒ åŠ è½½ {data_type} å¤±è´¥: {e}")
            loaded_data[data_type] = pd.DataFrame()   
    return (
        loaded_data.get("books", pd.DataFrame()),
        loaded_data.get("courses", pd.DataFrame()),
        loaded_data.get("news", pd.DataFrame()),
        loaded_data.get("notices", pd.DataFrame())  # æ–°å¢è¿”å›å€¼
    )
def clean_books(df):
    """æ¸…æ´—å›¾ä¹¦æ•°æ®"""
    if df.empty:
        print("ğŸ“š å›¾ä¹¦æ•°æ®ä¸ºç©º")
        return df    
    print(f"ğŸ§¹ æ¸…æ´—å›¾ä¹¦æ•°æ® ({len(df)}æ¡)...")    
    # å»é‡
    df = df.drop_duplicates()    
    # å¤„ç†å¹´ä»½
    if 'year' in df.columns:
        df['year'] = df['year'].astype(str)
        df['year_clean'] = df['year'].str.extract(r'(\d{4})', expand=False)
        df['year_clean'] = pd.to_numeric(df['year_clean'], errors='coerce')
        df['year_clean'] = df['year_clean'].fillna(2023).astype(int)    
    # ç¡®ä¿å¿…è¦åˆ—å­˜åœ¨
    required_columns = {
        'title': 'æœªå‘½åå›¾ä¹¦',
        'author': 'æœªçŸ¥ä½œè€…', 
        'category': 'æœªåˆ†ç±»',
        'publisher': 'æœªçŸ¥å‡ºç‰ˆç¤¾'
    }    
    for col, default in required_columns.items():
        if col not in df.columns:
            df[col] = default    
    print(f"âœ… å›¾ä¹¦æ¸…æ´—å®Œæˆ: {len(df)}æ¡")
    return df
def clean_courses(df):
    """æ¸…æ´—è¯¾ç¨‹æ•°æ®"""
    if df.empty:
        print("ğŸ“ è¯¾ç¨‹æ•°æ®ä¸ºç©º")
        return df    
    print(f"ğŸ§¹ æ¸…æ´—è¯¾ç¨‹æ•°æ® ({len(df)}æ¡)...")    
    df = df.drop_duplicates()    
    # å¤„ç†å­¦åˆ†
    if 'credit' in df.columns:
        df['credit'] = pd.to_numeric(df['credit'], errors='coerce')
        df['credit'] = df['credit'].fillna(2).astype(int)    
    # å¤„ç†å­¦æ—¶
    if 'hours' in df.columns:
        df['hours'] = pd.to_numeric(df['hours'], errors='coerce')
        df['hours'] = df['hours'].fillna(32).astype(int)    
    # ç¡®ä¿å¿…è¦åˆ—å­˜åœ¨
    required_columns = {
        'name': 'æœªå‘½åè¯¾ç¨‹',
        'teacher': 'æœªçŸ¥æ•™å¸ˆ',
        'department': 'æœªæŒ‡å®šé™¢ç³»',
        'code': 'æœªç¼–å·'
    }    
    for col, default in required_columns.items():
        if col not in df.columns:
            df[col] = default    
    print(f"âœ… è¯¾ç¨‹æ¸…æ´—å®Œæˆ: {len(df)}æ¡")
    return df
def clean_news(df):
    """æ¸…æ´—æ–°é—»æ•°æ®"""
    if df.empty:
        print("ğŸ“° æ–°é—»æ•°æ®ä¸ºç©º")
        return df    
    print(f"ğŸ§¹ æ¸…æ´—æ–°é—»æ•°æ® ({len(df)}æ¡)...")    
    df = df.drop_duplicates()    
    # å¤„ç†æ—¥æœŸ
    if 'date' in df.columns:
        df['date'] = df['date'].astype(str)
        df['date_clean'] = pd.to_datetime(df['date'], errors='coerce', format='mixed')
        df['date_clean'] = df['date_clean'].fillna(pd.Timestamp('2024-01-01'))   
    # å¤„ç†å†…å®¹é•¿åº¦
    if 'summary' in df.columns:
        df['summary'] = df['summary'].astype(str)
        df['summary_length'] = df['summary'].str.len()
    elif 'content' in df.columns:
        df['content'] = df['content'].astype(str)
        df['content_length'] = df['content'].str.len()   
    # ç¡®ä¿å¿…è¦åˆ—å­˜åœ¨
    required_columns = {
        'title': 'æœªå‘½åæ–°é—»',
        'category': 'ç»¼åˆæ–°é—»',
        'source': 'æœªçŸ¥æ¥æº'
    }   
    for col, default in required_columns.items():
        if col not in df.columns:
            df[col] = default   
    print(f"âœ… æ–°é—»æ¸…æ´—å®Œæˆ: {len(df)}æ¡")
    return df
def clean_notices(df):
    """æ¸…æ´—é€šçŸ¥å…¬å‘Šæ•°æ®ï¼ˆæ–°å¢å‡½æ•°ï¼‰"""
    if df.empty:
        print("ğŸ“¢ å…¬å‘Šæ•°æ®ä¸ºç©º")
        return df    
    print(f"ğŸ§¹ æ¸…æ´—å…¬å‘Šæ•°æ® ({len(df)}æ¡)...")    
    df = df.drop_duplicates()    
    # å¤„ç†æ—¥æœŸ
    if 'date' in df.columns:
        df['date'] = df['date'].astype(str)
        df['date_clean'] = pd.to_datetime(df['date'], errors='coerce', format='mixed')
        df['date_clean'] = df['date_clean'].fillna(pd.Timestamp('2024-01-01'))   
    # å¤„ç†å†…å®¹
    if 'content' in df.columns:
        df['content'] = df['content'].astype(str)
        df['content_length'] = df['content'].str.len()    
    # åˆ†ç±»å¤„ç†
    if 'category' not in df.columns:
        if 'type' in df.columns:
            df['category'] = df['type']
        else:
            df['category'] = 'é€šçŸ¥å…¬å‘Š'   
    # ç¡®ä¿å¿…è¦åˆ—å­˜åœ¨
    required_columns = {
        'title': 'æœªå‘½åé€šçŸ¥',
        'category': 'é€šçŸ¥å…¬å‘Š',
        'source': 'åŒ—äº¬å¤§å­¦ç›¸å…³éƒ¨é—¨'
    }   
    for col, default in required_columns.items():
        if col not in df.columns:
            df[col] = default    
    print(f"âœ… å…¬å‘Šæ¸…æ´—å®Œæˆ: {len(df)}æ¡")
    return df
def analyze_all_data(books, courses, news, notices):
    """åˆ†ææ‰€æœ‰æ•°æ®"""
    print("ğŸ“Š å¼€å§‹æ•°æ®åˆ†æ...")   
    analysis = {
        "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "summary": {
            "total_records": len(books) + len(courses) + len(news) + len(notices),
            "books_count": len(books),
            "courses_count": len(courses), 
            "news_count": len(news),
            "notices_count": len(notices)  # æ–°å¢
        },
        "books_analysis": {},
        "courses_analysis": {},
        "news_analysis": {},
        "notices_analysis": {}  # æ–°å¢
    }    
    # 1. å›¾ä¹¦åˆ†æ
    if not books.empty:
        if 'category' in books.columns:
            cat_counts = books['category'].value_counts().head(10)
            analysis["books_analysis"]["top_categories"] = cat_counts.to_dict()
        
        if 'year_clean' in books.columns:
            analysis["books_analysis"]["year_stats"] = {
                "average_year": int(books['year_clean'].mean()),
                "latest_year": int(books['year_clean'].max()),
                "year_range": f"{int(books['year_clean'].min())}-{int(books['year_clean'].max())}"
            }    
    # 2. è¯¾ç¨‹åˆ†æ
    if not courses.empty:
        if 'department' in courses.columns:
            dept_counts = courses['department'].value_counts().head(10)
            analysis["courses_analysis"]["top_departments"] = dept_counts.to_dict()
        
        if 'credit' in courses.columns:
            analysis["courses_analysis"]["credit_stats"] = {
                "average_credit": float(courses['credit'].mean()),
                "max_credit": int(courses['credit'].max()),
                "min_credit": int(courses['credit'].min())
            }    
    # 3. æ–°é—»åˆ†æ
    if not news.empty:
        if 'category' in news.columns:
            news_cat_counts = news['category'].value_counts().head(10)
            analysis["news_analysis"]["categories"] = news_cat_counts.to_dict()
        
        if 'date_clean' in news.columns:
            date_range = {
                "start": news['date_clean'].min().strftime("%Y-%m-%d"),
                "end": news['date_clean'].max().strftime("%Y-%m-%d")
            }
            analysis["news_analysis"]["date_range"] = date_range   
    # 4. å…¬å‘Šåˆ†æï¼ˆæ–°å¢ï¼‰
    if not notices.empty:
        if 'category' in notices.columns:
            notice_cat_counts = notices['category'].value_counts().head(10)
            analysis["notices_analysis"]["categories"] = notice_cat_counts.to_dict()
        
        if 'date_clean' in notices.columns:
            notice_dates = {
                "start": notices['date_clean'].min().strftime("%Y-%m-%d"),
                "end": notices['date_clean'].max().strftime("%Y-%m-%d"),
                "total_days": (notices['date_clean'].max() - notices['date_clean'].min()).days
            }
            analysis["notices_analysis"]["date_info"] = notice_dates       
        if 'content_length' in notices.columns:
            analysis["notices_analysis"]["content_stats"] = {
                "avg_length": int(notices['content_length'].mean()),
                "max_length": int(notices['content_length'].max()),
                "min_length": int(notices['content_length'].min())
            }   
    print("âœ… æ•°æ®åˆ†æå®Œæˆ")
    return analysis
def save_processed_data(books_clean, courses_clean, news_clean, notices_clean):
    """ä¿å­˜å¤„ç†åçš„æ•°æ®"""
    print("ğŸ’¾ ä¿å­˜å¤„ç†åçš„æ•°æ®...")    
    os.makedirs("data/processed", exist_ok=True)   
    # ä¿å­˜æ¯ç§æ•°æ®
    data_to_save = [
        ("books", books_clean),
        ("courses", courses_clean),
        ("news", news_clean),
        ("notices", notices_clean)  # æ–°å¢
    ]   
    for name, df in data_to_save:
        if not df.empty:
            file_path = f"data/processed/{name}_clean.csv"
            df.to_csv(file_path, index=False, encoding='utf-8-sig')
            print(f"   âœ… {name}: {len(df)}æ¡ -> {file_path}")    
    # åˆ›å»ºåˆå¹¶æ•°æ®é›†ï¼ˆç”¨äºåˆ†æï¼‰
    merged_data = []    
    for name, df in data_to_save:
        if not df.empty:
            # æ·»åŠ ç±»å‹æ ‡è¯†
            df_copy = df.copy()
            df_copy['data_type'] = name           
            # é€‰æ‹©é€šç”¨åˆ—
            common_cols = []
            for col in ['title', 'name', 'author', 'teacher', 'category', 'date', 'date_clean']:
                if col in df_copy.columns:
                    common_cols.append(col)            
            common_cols.append('data_type')            
            if common_cols:
                merged_data.append(df_copy[common_cols])    
    if merged_data:
        merged_df = pd.concat(merged_data, ignore_index=True)
        merged_df.to_csv("data/processed/merged_data.csv", index=False, encoding='utf-8-sig')
        print(f"   âœ… åˆå¹¶æ•°æ®: {len(merged_df)}æ¡ -> data/processed/merged_data.csv")
def save_analysis_results(analysis, books_clean, courses_clean, news_clean, notices_clean):
    """ä¿å­˜åˆ†æç»“æœå’Œæ ·æœ¬æ•°æ®"""
    print("ğŸ“„ ä¿å­˜åˆ†æç»“æœ...")    
    # è‡ªå®šä¹‰JSONåºåˆ—åŒ–å™¨
    def custom_serializer(obj):
        if isinstance(obj, (datetime, pd.Timestamp)):
            return obj.strftime("%Y-%m-%d %H:%M:%S")
        elif isinstance(obj, (np.integer, np.int64)):
            return int(obj)
        elif isinstance(obj, (np.floating, np.float64)):
            return float(obj)
        elif pd.isna(obj):
            return None
        raise TypeError(f"Type {type(obj)} not serializable")   
    # ä¿å­˜åˆ†æç»“æœ
    try:
        with open("data/analysis.json", "w", encoding='utf-8') as f:
            json.dump(analysis, f, ensure_ascii=False, indent=2, default=custom_serializer)
        print("   âœ… åˆ†æç»“æœ -> data/analysis.json")
    except Exception as e:
        print(f"   âš ï¸ ä¿å­˜åˆ†æç»“æœå¤±è´¥: {e}")   
    # ä¿å­˜æ•°æ®æ ·æœ¬ï¼ˆå‰100æ¡ï¼‰
    print("ğŸ“‹ ä¿å­˜æ•°æ®æ ·æœ¬...")   
    sample_data = {
        "sample_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
        "books_sample": books_clean.head(100).to_dict('records') if not books_clean.empty else [],
        "courses_sample": courses_clean.head(100).to_dict('records') if not courses_clean.empty else [],
        "news_sample": news_clean.head(100).to_dict('records') if not news_clean.empty else [],
        "notices_sample": notices_clean.head(100).to_dict('records') if not notices_clean.empty else []  # æ–°å¢
    }   
    # æ¸…ç†æ ·æœ¬æ•°æ®ä¸­çš„éåºåˆ—åŒ–å¯¹è±¡
    def clean_dict_list(data_list):
        cleaned = []
        for item in data_list:
            cleaned_item = {}
            for key, value in item.items():
                if isinstance(value, (datetime, pd.Timestamp)):
                    cleaned_item[key] = value.strftime("%Y-%m-%d %H:%M:%S")
                elif pd.isna(value):
                    cleaned_item[key] = None
                elif isinstance(value, (np.integer, np.int64)):
                    cleaned_item[key] = int(value)
                elif isinstance(value, (np.floating, np.float64)):
                    cleaned_item[key] = float(value)
                else:
                    cleaned_item[key] = value
            cleaned.append(cleaned_item)
        return cleaned   
    for key in ['books_sample', 'courses_sample', 'news_sample', 'notices_sample']:
        if sample_data[key]:
            sample_data[key] = clean_dict_list(sample_data[key])    
    try:
        with open("data/samples.json", "w", encoding='utf-8') as f:
            json.dump(sample_data, f, ensure_ascii=False, indent=2, default=custom_serializer)
        print("   âœ… æ•°æ®æ ·æœ¬ -> data/samples.json")
    except Exception as e:
        print(f"   âš ï¸ ä¿å­˜æ ·æœ¬å¤±è´¥: {e}")
def run_processing():
    """è¿è¡Œæ•°æ®å¤„ç†æµç¨‹"""
    print("\n" + "=" * 60)
    print("æ•°æ®å¤„ç†æµç¨‹")
    print("=" * 60)   
    start_time = time.time() if 'time' in globals() else datetime.now().timestamp()   
    try:
        # 1. åŠ è½½æ•°æ®
        books, courses, news, notices = load_data()       
        if books.empty and courses.empty and news.empty and notices.empty:
            print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½•æ•°æ®æ–‡ä»¶")
            return None    
        # 2. æ¸…æ´—æ•°æ®
        print("\n" + "-" * 40)
        print("æ•°æ®æ¸…æ´—")
        print("-" * 40)        
        books_clean = clean_books(books)
        courses_clean = clean_courses(courses)
        news_clean = clean_news(news)
        notices_clean = clean_notices(notices)  # æ–°å¢        
        # 3. ä¿å­˜å¤„ç†åçš„æ•°æ®
        print("\n" + "-" * 40)
        print("ä¿å­˜æ•°æ®")
        print("-" * 40)      
        save_processed_data(books_clean, courses_clean, news_clean, notices_clean)      
        # 4. åˆ†ææ•°æ®
        print("\n" + "-" * 40)
        print("æ•°æ®åˆ†æ")
        print("-" * 40)       
        analysis = analyze_all_data(books_clean, courses_clean, news_clean, notices_clean)      
        # 5. ä¿å­˜åˆ†æç»“æœ
        print("\n" + "-" * 40)
        print("ä¿å­˜ç»“æœ")
        print("-" * 40)      
        save_analysis_results(analysis, books_clean, courses_clean, news_clean, notices_clean)       
        # 6. æ˜¾ç¤ºç»Ÿè®¡ä¿¡æ¯
        total_time = (datetime.now().timestamp() - start_time) if 'time' in globals() else 0      
        print("\n" + "=" * 60)
        print("âœ… æ•°æ®å¤„ç†å®Œæˆ!")
        print("=" * 60)        
        print(f"\nğŸ“Š æ•°æ®ç»Ÿè®¡:")
        print(f"   å›¾ä¹¦æ•°æ®: {len(books_clean)}æ¡")
        print(f"   è¯¾ç¨‹æ•°æ®: {len(courses_clean)}æ¡")
        print(f"   æ–°é—»æ•°æ®: {len(news_clean)}æ¡")
        print(f"   å…¬å‘Šæ•°æ®: {len(notices_clean)}æ¡")
        print(f"   æ€»è®¡: {analysis['summary']['total_records']}æ¡")        
        print(f"\nâ±ï¸  å¤„ç†è€—æ—¶: {total_time:.2f}ç§’")
        print(f"ğŸ“ è¾“å‡ºç›®å½•: data/processed/")
        print(f"ğŸ“„ åˆ†ææ–‡ä»¶: data/analysis.json")
        print(f"ğŸ“‹ æ ·æœ¬æ–‡ä»¶: data/samples.json")
        print("=" * 60)     
        return analysis       
    except Exception as e:
        print(f"\nâŒ æ•°æ®å¤„ç†å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None
# æ·»åŠ timeæ¨¡å—å¯¼å…¥ï¼ˆå¦‚æœéœ€è¦ï¼‰
import time
if __name__ == "__main__":
    run_processing()